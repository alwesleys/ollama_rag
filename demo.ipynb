{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "\n",
    "### Python        - install via windows store\n",
    "### Ollama        - https://ollama.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup the Virtual Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 -m venv venv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activate venv - windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "! .\\venv\\Scripts\\activate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activate venv - linux / mac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! source venv/bin/activate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code - Setup the LLM\n",
    "\n",
    "### REFERENCE DOC - https://python.langchain.com/docs/introduction/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define local llm\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# use model name running in ollama localy\n",
    "# List all available models:    ollama list\n",
    "# Run specific model:           ollama run llama3.2:latest\n",
    "# Check running model:          ollama ps\n",
    "# https://ollama.com/search\n",
    "llm = ChatOllama(model=\"llama3.2:latest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n",
    "    ),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "ai_msg = llm.invoke(messages)\n",
    "ai_msg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code - Prepare document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define embeddings to convert documents to vectors\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "# can use specialize models for embeddings\n",
    "# https://ollama.com/search?c=embedding\n",
    "embeddings = OllamaEmbeddings( model=\"llama3.2:latest\")\n",
    "\n",
    "# Define vector store to index documents\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "vector_store = InMemoryVectorStore(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define document loader to load PDFs\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "\n",
    "# Load PDF document\n",
    "docs = []\n",
    "loader = PyPDFDirectoryLoader(\"data/\")\n",
    "docs_lazy = loader.lazy_load()\n",
    "for doc in docs_lazy:\n",
    "    docs.append(doc)\n",
    "\n",
    "# Index documents\n",
    "vector_store = InMemoryVectorStore.from_documents(docs,embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check loaded docuent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "print(len(docs))\n",
    "pprint.pp(docs[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Prompt - How should the AI answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prompt for question-answering\n",
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# You are an assistant for question-answering tasks. \n",
    "# Use the following pieces of retrieved context to answer the question. \n",
    "# If you don't know the answer, just say that you don't know. \n",
    "# Use three sentences maximum and keep the answer concise.\n",
    "# Question: {question} \n",
    "# Context: {context} \n",
    "# Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retriever - Getting the Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import List, TypedDict\n",
    "from langchain_core.documents import Document\n",
    "from langgraph.graph import START, StateGraph\n",
    "\n",
    "# Define state for application\n",
    "# being updated by Retriever and Generator steps\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "# Define application steps\n",
    "# Retrieve top documents close to the question as context\n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invoke graph by passing question (required by retrieve step)\n",
    "state = State(\n",
    "    question=\"What does response code of 91 mean?\",\n",
    "    context=[],\n",
    "    answer=\"\"\n",
    ")\n",
    "\n",
    "response = retrieve(state);\n",
    "pprint.pp(response.get(\"context\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator - Generate Answer based on Context + Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate answer using retrieved documents in context\n",
    "# State contains the question and retrieved documents\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invoke graph by passing question (required by retrieve step)\n",
    "state = State(\n",
    "    question=\"What does response code of 61 mean?\",\n",
    "    context= retrieve(state).get(\"context\"), # retrieved from previous step\n",
    "    answer=\"\"\n",
    ")\n",
    "\n",
    "response = generate(state)\n",
    "pprint.pp(response.get(\"answer\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Graph - Retrieve > Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup graph (retrieve -> generate)\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invoke graph by passing question (required by retrieve step)\n",
    "response = graph.invoke({\"question\": \"What does response code of 91 mean?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Model size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://enclaveai.app/blog/2024/05/13/understanding-llm-model-sizes/\n",
    "# llm = ChatOllama(model=\"llama3.2:1b\") # use model with 1b params\n",
    "llm = ChatOllama(model=\"llama3.2:latest\") # use model with 7b params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo on implementing a Chat Interface\n",
    "\n",
    "### REFERENCE DOC - https://docs.chainlit.io/get-started/overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install chainlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! chainlit run app.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
